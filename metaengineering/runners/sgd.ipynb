{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tijmen/tudelft/thesis/metaengineering\n"
     ]
    }
   ],
   "source": [
    "cd /home/tijmen/tudelft/thesis/metaengineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tijmen/tudelft/thesis/.env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from src.utils.utils import get_generator, get_project_root\n",
    "from src.pipeline.config import DataLoaderConfig, TaskLoaderConfig\n",
    "from src.pipeline.taskloader import TaskLoader, TaskFrame\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from src.settings.tier import Tier\n",
    "from src.settings.strategy import Strategy\n",
    "from src.orchestrator.orchestrator import Orchestrator\n",
    "from src.pipeline.dataloader import DataLoader\n",
    "from src.orchestrator.trainer import Trainer\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader.DATA_FOLDER = f'{get_project_root()}/data/training/'\n",
    "\n",
    "tier = Tier.TIER0\n",
    "strategy = Strategy.ALL\n",
    "\n",
    "dl_config = DataLoaderConfig(\n",
    "    additional_filters=[\"is_precursor\", ],\n",
    "    additional_transforms=[\"log_fold_change_protein\", ]\n",
    ")\n",
    "\n",
    "tl_config = TaskLoaderConfig(\n",
    "    data_throttle=1,\n",
    "    tier=tier,\n",
    ")\n",
    "\n",
    "dl = DataLoader()\n",
    "dl.prepare_dataloader(dl_config)\n",
    "\n",
    "tl = TaskLoader()\n",
    "tl.prepare_taskloader(tl_config)\n",
    "\n",
    "gen = get_generator(dl, tl, strategy, tier)\n",
    "tf = next(gen)\n",
    "\n",
    "trainer = Trainer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_kwargs = dict(\n",
    "    stratify='metabolite_id',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = trainer.do_train_test_split(tf, strategy, **split_kwargs)\n",
    "metabolite_id = y_test.index.get_level_values('metabolite_id')\n",
    "\n",
    "X_train = X_train.drop(['KO_ORF', 'metabolite_id'], axis=1)\n",
    "X_test = X_test.drop(['KO_ORF', 'metabolite_id'], axis=1)\n",
    "\n",
    "X_train = x_scaler.fit_transform(X_train)\n",
    "X_test = x_scaler.transform(X_test)\n",
    "\n",
    "y_train = y_scaler.fit_transform(y_train.values.reshape((-1, 1)))\n",
    "y_test = y_scaler.transform(y_test.values.reshape((-1, 1)))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = map(\n",
    "    torch.from_numpy, (\n",
    "        X_train.astype(np.float32), X_test.astype(np.float32), \n",
    "        y_train.astype(np.float32), y_test.astype(np.float32)\n",
    "    )\n",
    ")\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "test_ds = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, test_ds, bs):\n",
    "    return (\n",
    "        TorchDataLoader(train_ds, bs, shuffle=True),\n",
    "        TorchDataLoader(test_ds)\n",
    "    )\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "    if opt is not None:\n",
    "        loss.backward(),\n",
    "        opt.step(),\n",
    "        opt.zero_grad()\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, test_dl, writer: SummaryWriter):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (xb, yb) in enumerate(train_dl, 0):\n",
    "            loss, _ = loss_batch(model, loss_func, xb, yb, opt)\n",
    "            running_loss += loss\n",
    "            \n",
    "            if i % 10 == 9:\n",
    "                avg_loss = running_loss / 10\n",
    "                writer.add_scalars('Training loss', {'Training': avg_loss}, epoch * len(train_dl) + i)\n",
    "        model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1]\n",
    "n_outputs = 1\n",
    "num_nodes_hidden = [64, 32]\n",
    "lr = 0.001\n",
    "        \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=n_features, out_features=num_nodes_hidden[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=num_nodes_hidden[0], out_features=num_nodes_hidden[1]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=num_nodes_hidden[1], out_features=n_outputs),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "loss_func = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = get_data(train_ds, test_ds, 128)\n",
    "writer = SummaryWriter(log_dir=f\"{get_project_root()}/runners/runs\")\n",
    "\n",
    "fit(100, model, loss_func, opt, train_dl, test_dl, writer)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, test_dl = get_data(train_ds, test_ds, 128)\n",
    "dataiter = iter(train_dl)\n",
    "\n",
    "samples, result = next(dataiter)\n",
    "writer.add_graph(model, samples)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall mae: 0.11\n",
      "==================================== \n",
      "pyr mae: 0.09, \n",
      "r2-score: 0.82, \n",
      "min prediction: 0.36, max prediction: 0.86 \n",
      "mean log change: 0.70, mean prediction 0.66 \n",
      "Inverse scaled: 0.82\n",
      "==================================== \n",
      "r5p mae: 0.09, \n",
      "r2-score: 0.78, \n",
      "min prediction: 0.36, max prediction: 0.83 \n",
      "mean log change: 0.74, mean prediction 0.69 \n",
      "Inverse scaled: 0.78\n",
      "==================================== \n",
      "pep mae: 0.10, \n",
      "r2-score: 0.72, \n",
      "min prediction: 0.36, max prediction: 0.83 \n",
      "mean log change: 0.70, mean prediction 0.68 \n",
      "Inverse scaled: 0.72\n",
      "==================================== \n",
      "3pg;2pg mae: 0.05, \n",
      "r2-score: 0.83, \n",
      "min prediction: 0.44, max prediction: 0.86 \n",
      "mean log change: 0.69, mean prediction 0.69 \n",
      "Inverse scaled: 0.83\n",
      "==================================== \n",
      "dhap mae: 0.09, \n",
      "r2-score: 0.90, \n",
      "min prediction: 0.36, max prediction: 0.81 \n",
      "mean log change: 0.66, mean prediction 0.63 \n",
      "Inverse scaled: 0.90\n",
      "==================================== \n",
      "f6p mae: 0.13, \n",
      "r2-score: 0.85, \n",
      "min prediction: 0.36, max prediction: 0.86 \n",
      "mean log change: 0.66, mean prediction 0.64 \n",
      "Inverse scaled: 0.85\n",
      "==================================== \n",
      "g6p;f6p;g6p-B mae: 0.18, \n",
      "r2-score: 0.24, \n",
      "min prediction: 0.36, max prediction: 0.83 \n",
      "mean log change: 0.53, mean prediction 0.67 \n",
      "Inverse scaled: 0.24\n",
      "==================================== \n",
      "accoa mae: 0.02, \n",
      "r2-score: 0.83, \n",
      "min prediction: 0.36, max prediction: 0.45 \n",
      "mean log change: 0.44, mean prediction 0.42 \n",
      "Inverse scaled: 0.83\n",
      "==================================== \n",
      "g6p;g6p-B mae: 0.21, \n",
      "r2-score: -0.11, \n",
      "min prediction: 0.36, max prediction: 0.39 \n",
      "mean log change: 0.59, mean prediction 0.37 \n",
      "Inverse scaled: -0.11\n",
      "==================================== \n",
      "akg mae: 0.23, \n",
      "r2-score: -0.13, \n",
      "min prediction: 0.36, max prediction: 0.47 \n",
      "mean log change: 0.66, mean prediction 0.43 \n",
      "Inverse scaled: -0.13\n",
      "==================================== \n",
      "oaa mae: 0.09, \n",
      "r2-score: 0.48, \n",
      "min prediction: 0.36, max prediction: 0.51 \n",
      "mean log change: 0.52, mean prediction 0.43 \n",
      "Inverse scaled: 0.48\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predictions = [model(xb).cpu().numpy().squeeze().item() for xb, _ in test_dl]\n",
    "    print(f\"Overall mae: {mean_absolute_error(y_test, predictions):.2f}\")\n",
    "    for metabolite in metabolite_id.unique():\n",
    "        metabolite_predictions = []\n",
    "        target = []\n",
    "        for (xb, yb), is_current_metabolite in zip(test_dl, metabolite_id == metabolite):\n",
    "            if is_current_metabolite:\n",
    "                metabolite_predictions.append(model(xb).cpu().numpy().squeeze().item())\n",
    "                target.append(yb.cpu().numpy().squeeze().item())\n",
    "        print(\n",
    "            f\"==================================== \\n\"\n",
    "            f\"{metabolite} mae: {mean_absolute_error(target, metabolite_predictions):.2f}, \\n\"\n",
    "            f\"r2-score: {pearsonr(target, metabolite_predictions)[0]:.2f}, \\n\"\n",
    "            f\"min prediction: {min(metabolite_predictions):.2f}, max prediction: {max(metabolite_predictions):.2f} \\n\"\n",
    "            f\"mean log change: {np.mean(target):.2f}, mean prediction {np.mean(metabolite_predictions):.2f} \\n\"\n",
    "            f\"Inverse scaled: {pearsonr(y_scaler.inverse_transform(np.asarray(target).reshape((-1, 1))).squeeze(), y_scaler.inverse_transform(np.asarray(metabolite_predictions).reshape((-1, 1))).squeeze())[0]:.2f}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4de30f81a9e56b051823f8e1b81de8d62bfaa39f562f681b696ca2c9e1b37dd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
