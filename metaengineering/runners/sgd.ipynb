{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tvangraft/tudelft/thesis/metaengineering\n"
     ]
    }
   ],
   "source": [
    "cd /home/tvangraft/tudelft/thesis/metaengineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "from src.utils.utils import get_generator, get_project_root\n",
    "from src.utils.test_result_store import TestResultStore\n",
    "\n",
    "from src.pipeline.config import DataLoaderConfig, TaskLoaderConfig\n",
    "from src.pipeline.taskloader import TaskLoader, TaskFrame\n",
    "from src.pipeline.dataloader import DataLoader\n",
    "\n",
    "from src.settings.tier import Tier\n",
    "from src.settings.strategy import Strategy\n",
    "\n",
    "from src.orchestrator.orchestrator import SklearnOrchestrator\n",
    "from src.orchestrator.trainer import Trainer\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader as TorchDataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader.DATA_FOLDER = f'{get_project_root()}/data/training/'\n",
    "\n",
    "tier = Tier.TIER0\n",
    "strategy = Strategy.ALL\n",
    "\n",
    "dl_config = DataLoaderConfig(\n",
    "    additional_filters=[\"is_precursor\", ],\n",
    "    additional_transforms=[\"log_fold_change_protein\", ]\n",
    ")\n",
    "\n",
    "tl_config = TaskLoaderConfig(\n",
    "    data_throttle=1,\n",
    "    tier=tier,\n",
    ")\n",
    "\n",
    "dl = DataLoader()\n",
    "dl.prepare_dataloader(dl_config)\n",
    "\n",
    "tl = TaskLoader()\n",
    "tl.prepare_taskloader(tl_config)\n",
    "\n",
    "gen = get_generator(dl, tl, strategy, tier)\n",
    "tf = next(gen)\n",
    "\n",
    "trainer = Trainer()\n",
    "\n",
    "split_kwargs = dict(\n",
    "    stratify='metabolite_id',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "X_train, X_test, y_train, y_test = trainer.do_train_test_split(tf, strategy, **split_kwargs)\n",
    "metabolite_id = y_test.index.get_level_values('metabolite_id')\n",
    "\n",
    "X_train = X_train.drop(['KO_ORF', 'metabolite_id'], axis=1)\n",
    "# X_test = X_test.drop(['KO_ORF', 'metabolite_id'], axis=1)\n",
    "\n",
    "X_train = x_scaler.fit_transform(X_train)\n",
    "# X_test = x_scaler.transform(X_test)\n",
    "\n",
    "y_train = y_scaler.fit_transform(y_train.values.reshape((-1, 1)))\n",
    "# y_test = y_scaler.transform(y_test.values.reshape((-1, 1)))\n",
    "\n",
    "\n",
    "X_train, y_train = map(\n",
    "    torch.from_numpy, (\n",
    "        X_train.astype(np.float32), \n",
    "        y_train.astype(np.float32)\n",
    "    )\n",
    ")\n",
    "\n",
    "train_ds = TensorDataset(X_train, y_train)\n",
    "# test_ds = TensorDataset(X_test, y_test)\n",
    "len(X_test['KO_ORF'].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['pyr', 'g6p;f6p;g6p-B', '3pg;2pg', 'dhap', 'akg', 'oaa', 'r5p',\n",
       "       'pep', 'e4p', 'f6p', 'accoa', 'g6p;g6p-B'], dtype=object)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataLoader.DATA_FOLDER = f'{get_project_root()}/data/training/'\n",
    "\n",
    "tier = Tier.TIER0\n",
    "strategy = Strategy.ALL\n",
    "\n",
    "dl_config = DataLoaderConfig(\n",
    "    additional_filters=[\"is_precursor\"],\n",
    "    additional_transforms=[\"log_fold_change_protein\", ]\n",
    ")\n",
    "\n",
    "tl_config = TaskLoaderConfig(\n",
    "    data_throttle=1,\n",
    "    tier=tier,\n",
    ")\n",
    "\n",
    "dl = DataLoader()\n",
    "dl.prepare_dataloader(dl_config)\n",
    "\n",
    "tl = TaskLoader()\n",
    "tl.prepare_taskloader(tl_config)\n",
    "\n",
    "gen = get_generator(dl, tl, strategy, tier)\n",
    "tf = next(gen)\n",
    "\n",
    "trainer = Trainer()\n",
    "split_kwargs = dict(shuffle=True, stratify='metabolite_id')\n",
    "X_train, X_test, y_train, y_test = trainer.do_train_test_split(tf, strategy, **split_kwargs)\n",
    "# metabolite_id = y_test.index.get_level_values('metabolite_id')\n",
    "\n",
    "\n",
    "\n",
    "# X_train = X_train.drop(['KO_ORF', 'metabolite_id'], axis=1)\n",
    "# # X_test = X_test.drop(['KO_ORF', 'metabolite_id'], axis=1)\n",
    "\n",
    "# X_train = x_scaler.fit_transform(X_train)\n",
    "# # X_test = x_scaler.transform(X_test)\n",
    "\n",
    "# y_train = y_scaler.fit_transform(y_train.values.reshape((-1, 1)))\n",
    "# # y_test = y_scaler.transform(y_test.values.reshape((-1, 1)))\n",
    "\n",
    "\n",
    "# X_train, y_train = map(\n",
    "#     torch.from_numpy, (\n",
    "#         X_train.astype(np.float32), \n",
    "#         y_train.astype(np.float32)\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# train_ds = TensorDataset(X_train, y_train)\n",
    "# # test_ds = TensorDataset(X_test, y_test)\n",
    "\n",
    "X_test['metabolite_id'].unique()\n",
    "# len(X_test['KO_ORF'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    x_scaler: StandardScaler,\n",
    "    y_scaler: MinMaxScaler,\n",
    "    metabolite_id: str = \"\"\n",
    "):\n",
    "    _X_test = X_test[X_test['metabolite_id'] == metabolite_id].copy() if len(metabolite_id) > 0 else X_test.copy()\n",
    "    _y_test = y_test.xs(metabolite_id, level=\"metabolite_id\").copy() if len(metabolite_id) > 0 else y_test.copy()\n",
    "\n",
    "    _X_test = _X_test.drop(['KO_ORF', 'metabolite_id'], axis=1)\n",
    "    _X_test = x_scaler.transform(_X_test)\n",
    "\n",
    "    _y_test = y_scaler.transform(_y_test.values.reshape((-1, 1)))\n",
    "    _X_test,  _y_test = map(\n",
    "        torch.from_numpy, (\n",
    "            _X_test.astype(np.float32), _y_test.astype(np.float32),\n",
    "        )\n",
    "    )\n",
    "    test_ds = TensorDataset(_X_test, _y_test)\n",
    "    return TorchDataLoader(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(train_ds, bs):\n",
    "    return TorchDataLoader(train_ds, bs, shuffle=True)\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    loss = loss_func(model(xb), yb)\n",
    "    if opt is not None:\n",
    "        loss.backward(),\n",
    "        opt.step(),\n",
    "        opt.zero_grad()\n",
    "    return loss.item(), len(xb)\n",
    "\n",
    "def fit(epochs, model, loss_func, opt, train_dl, writer: SummaryWriter):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (xb, yb) in enumerate(train_dl, 0):\n",
    "            loss, _ = loss_batch(model, loss_func, xb, yb, opt)\n",
    "            running_loss += loss\n",
    "            \n",
    "            if i % 10 == 9:\n",
    "                avg_loss = running_loss / 10\n",
    "                writer.add_scalars('Training loss', {'Training': avg_loss}, epoch * len(train_dl) + i)\n",
    "        model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X_train.shape[1]\n",
    "n_outputs = 1\n",
    "num_nodes_hidden = [64, 32]\n",
    "lr = 0.001\n",
    "        \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=n_features, out_features=num_nodes_hidden[0]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=num_nodes_hidden[0], out_features=num_nodes_hidden[1]),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=num_nodes_hidden[1], out_features=n_outputs),\n",
    ")\n",
    "\n",
    "opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "loss_func = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = get_data(train_ds, 128)\n",
    "writer = SummaryWriter(log_dir=f\"{get_project_root()}/runners/runs\")\n",
    "\n",
    "fit(100, model, loss_func, opt, train_dl, writer)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl= get_data(train_ds, 128)\n",
    "dataiter = iter(train_dl)\n",
    "\n",
    "samples, result = next(dataiter)\n",
    "writer.add_graph(model, samples)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "testResultStore = TestResultStore(\n",
    "    experiment_path=f\"{get_project_root()}/data/results/{Tier.TIER0}\",\n",
    "    strategy=Strategy.ALL,\n",
    "    runner='sgd'\n",
    ")\n",
    "architecture = 'sgd'\n",
    "\n",
    "def get_predict_fn(model, y_scaler: MinMaxScaler):\n",
    "    def predict_fn(test_dl: TorchDataLoader):\n",
    "        predictions = np.asarray([model(xb).cpu().numpy().squeeze().item() for xb, _ in test_dl])\n",
    "        return y_scaler.inverse_transform(predictions.reshape((-1, 1))).squeeze()\n",
    "    return predict_fn\n",
    "\n",
    "def get_gt(test_dl: TorchDataLoader, y_scaler: MinMaxScaler):\n",
    "    _y_test = np.asarray([yb.cpu().numpy().squeeze().item() for _, yb in test_dl])\n",
    "    _y_test = y_scaler.inverse_transform(_y_test.reshape((-1, 1))).squeeze()\n",
    "    return pd.Series(_y_test)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predict_fn = get_predict_fn(model, y_scaler)\n",
    "    test_dl = get_test_data(X_test, y_test, x_scaler, y_scaler)\n",
    "    \n",
    "    testResultStore.update_results(\n",
    "        key='all', predict_fn=predict_fn, architecture='sgd', X_test=test_dl, y_test=get_gt(test_dl, y_scaler)\n",
    "    )\n",
    "    \n",
    "    for metabolite in metabolite_id.unique():\n",
    "        test_dl = get_test_data(X_test, y_test, x_scaler, y_scaler, metabolite)\n",
    "        testResultStore.update_results(\n",
    "            key=metabolite, predict_fn=predict_fn, architecture='sgd', X_test=test_dl, y_test=get_gt(test_dl, y_scaler)\n",
    "        )\n",
    "\n",
    "testResultStore.to_file()\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     model.eval()\n",
    "#     predictions = [model(xb).cpu().numpy().squeeze().item() for xb, _ in test_dl]\n",
    "#     print(f\"Overall mae: {mean_absolute_error(y_test, predictions):.2f}\")\n",
    "#     for metabolite in metabolite_id.unique():\n",
    "#         metabolite_predictions = []\n",
    "#         target = []\n",
    "#         for (xb, yb), is_current_metabolite in zip(test_dl, metabolite_id == metabolite):\n",
    "#             if is_current_metabolite:\n",
    "#                 metabolite_predictions.append(model(xb).cpu().numpy().squeeze().item())\n",
    "#                 target.append(yb.cpu().numpy().squeeze().item())\n",
    "#         print(\n",
    "#             f\"==================================== \\n\"\n",
    "#             f\"{metabolite} mae: {mean_absolute_error(target, metabolite_predictions):.2f}, \\n\"\n",
    "#             f\"r2-score: {pearsonr(target, metabolite_predictions)[0]:.2f}, \\n\"\n",
    "#             f\"min prediction: {min(metabolite_predictions):.2f}, max prediction: {max(metabolite_predictions):.2f} \\n\"\n",
    "#             f\"mean log change: {np.mean(target):.2f}, mean prediction {np.mean(metabolite_predictions):.2f} \\n\"\n",
    "#             f\"Inverse scaled: {pearsonr(y_scaler.inverse_transform(np.asarray(target).reshape((-1, 1))).squeeze(), y_scaler.inverse_transform(np.asarray(metabolite_predictions).reshape((-1, 1))).squeeze())[0]:.2f}\"\n",
    "#         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a542b6343c6490b05f36876d198b1a03a25da0ffb80d837b6b25e87db0e241d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
