{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tvangraft/tudelft/thesis/metaengineering\n"
     ]
    }
   ],
   "source": [
    "cd /home/tvangraft/tudelft/thesis/metaengineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tvangraft/tudelft/thesis/.env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe1aa538730>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from typing import DefaultDict, List, Hashable, Dict, Any\n",
    "\n",
    "from src.utils.utils import get_generator, get_project_root\n",
    "from src.utils.test_result_store import TestResultStore\n",
    "\n",
    "from src.pipeline.config import DataLoaderConfig, TaskLoaderConfig\n",
    "from src.pipeline.taskloader import TaskLoader, TaskFrame\n",
    "from src.pipeline.dataloader import DataLoader\n",
    "\n",
    "from src.orchestrator.trainer import Trainer\n",
    "\n",
    "from src.settings.tier import Tier\n",
    "from src.settings.strategy import Strategy\n",
    "from src.settings.metabolites import ENZYMES, METABOLITES, PRECURSOR_METABOLITES, PRECURSOR_METABOLITES_NO_TRANSFORM\n",
    "\n",
    "from src.gnn.data_augmentation import DataAugmentation\n",
    "from src.gnn.embeddings import generate_embedding\n",
    "from src.gnn.graph_builder import get_samples_hetero_graph, get_graph_fc, get_graph_fc_protein_only, edge_index_from_df_protein_only, get_samples_graph\n",
    "from src.gnn.shared_training import count_parameters, log_metrics, tune_metabolite_hyper_parameters\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cobra\n",
    "from cobra.util import create_stoichiometric_matrix\n",
    "from cobra.core import Reaction\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from more_itertools import flatten\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.nn import BatchNorm1d, ModuleList\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.utils import from_networkx, to_networkx\n",
    "from torch_geometric.loader import DataLoader as GeoDataLoader\n",
    "from torch_geometric.nn import GAT, GCNConv, to_hetero, SAGEConv, GATConv, HeteroLinear, Linear, Node2Vec\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn.conv import HeteroConv\n",
    "\n",
    "import mlflow.pytorch\n",
    "\n",
    "from config import HYPERPARAMETERS, BEST_PARAMETERS\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler, FIFOScheduler\n",
    "from ray.air import session, RunConfig\n",
    "from ray.tune.integration.mlflow import mlflow_mixin\n",
    "from ray.tune.integration.mlflow import MLflowLoggerCallback\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "device = torch.device(\"cpu\")\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/tvangraft/tudelft/thesis/metaengineering/data\"\n",
    "model = cobra.io.read_sbml_model(f'{path}/iMM904.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(path: str, valid_metabolites: List[str]):\n",
    "    edge_list_df = pd.read_csv(path)\n",
    "    graph_fc = get_graph_fc_protein_only(edge_list_df, valid_metabolites)\n",
    "    edge_index = edge_index_from_df_protein_only(graph_fc, edge_list_df, valid_metabolites)\n",
    "    embedding = generate_embedding(edge_index.T, device, hetero=False)\n",
    "\n",
    "    return edge_list_df, graph_fc, edge_index, embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATModel(torch.nn.Module):\n",
    "    def __init__(self, model_config) -> None:\n",
    "        super(GATModel, self).__init__()\n",
    "        embedding_size = model_config[\"model_embedding_size\"]\n",
    "        n_heads = model_config[\"model_attention_heads\"]\n",
    "        self.n_layers = model_config[\"model_layers\"]\n",
    "        \n",
    "        self.conv_layers = ModuleList([])\n",
    "        self.transf_layers = ModuleList([])\n",
    "        self.pooling_layers = ModuleList([])\n",
    "        self.bn_layers = ModuleList([])\n",
    "        \n",
    "        self.conv1 = GATConv(\n",
    "            -1, out_channels=embedding_size, heads=n_heads, add_self_loops=False, bias=False\n",
    "        )\n",
    "        self.transf1 = Linear(\n",
    "            in_channels=embedding_size*n_heads, \n",
    "            out_channels=embedding_size, \n",
    "        )\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            self.conv_layers.append(\n",
    "                GATConv(\n",
    "                    -1, \n",
    "                    out_channels=embedding_size, \n",
    "                    heads=n_heads, \n",
    "                    add_self_loops=False\n",
    "                )\n",
    "            )\n",
    "            self.transf_layers.append(\n",
    "                Linear(\n",
    "                    embedding_size*n_heads, \n",
    "                    embedding_size\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.linear1 = Linear(embedding_size * 2, embedding_size)\n",
    "        self.linear2 = Linear(embedding_size, model_config['output_size'])\n",
    "    \n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "\n",
    "        global_representation = []\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.conv_layers[i](x, edge_index)\n",
    "            print(x.shape)\n",
    "            x = torch.relu(self.transf_layers[i](x))\n",
    "            print(x.shape)\n",
    "            global_representation.append(torch.cat([gmp(x, batch_index), gap(x, batch_index)], dim=1))\n",
    "\n",
    "        # This generates the last embeddings for all enzymes in the graph\n",
    "        x = sum(global_representation)\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = F.dropout(x, p=0.8, training=self.training)\n",
    "        x = self.linear2(x)\n",
    "        return x "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch, model, train_loader, optimizer, loss_fn, debug=False):\n",
    "    # Enumerate over the data\n",
    "    running_loss = 0.0\n",
    "    step = 0\n",
    "    for _, batch in enumerate(train_loader):\n",
    "        # Use GPU\n",
    "        batch.to(device)\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad() \n",
    "        # Passing the node features and the connection info\n",
    "        pred = model.forward(\n",
    "            batch.x, \n",
    "            batch.edge_index,\n",
    "            batch.batch\n",
    "        )\n",
    "        # Calculating the loss and gradients\n",
    "        train_mask = torch.reshape(batch.train_mask.bool(), pred.shape)\n",
    "\n",
    "        if debug:\n",
    "            print(batch)\n",
    "            print(f\"{pred.shape=}\")\n",
    "            print(f\"{train_mask.sum()=}\")\n",
    "            print(\n",
    "                f\"{pred.shape=} \\n\"\n",
    "                f\"{pred[train_mask].mean()=} \\n\"\n",
    "                f\"{pred[train_mask].max()=} \\n\"\n",
    "                f\"{pred[train_mask].min()=} \\n\"\n",
    "                f\"{pred[train_mask].shape=} \\n\"\n",
    "            )\n",
    "            print(\n",
    "                f\"{torch.squeeze(pred).shape} \\n\"\n",
    "                f\"{torch.squeeze(batch.y.float()).shape} \\n\"\n",
    "            )\n",
    "\n",
    "        loss = loss_fn(\n",
    "            torch.squeeze(pred), \n",
    "            torch.squeeze(torch.reshape(batch.y.float(), pred.shape))\n",
    "        )\n",
    "\n",
    "        loss.backward()  \n",
    "        optimizer.step()  \n",
    "        # Update tracking\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "    \n",
    "    tune.report(loss=(running_loss/step))\n",
    "\n",
    "    return running_loss/step\n",
    "    \n",
    "@mlflow_mixin\n",
    "def test(epoch, model, test_loader, loss_fn, debug=False):\n",
    "    all_preds_raw = []\n",
    "    all_labels = []\n",
    "    all_knockout_ids = []\n",
    "    running_loss = 0.0\n",
    "    step = 0\n",
    "    for batch in test_loader:\n",
    "        batch.to(device)\n",
    "        pred = model(\n",
    "            batch.x, \n",
    "            batch.edge_index,\n",
    "            batch.batch,\n",
    "        )\n",
    "        test_mask = torch.reshape(batch.test_mask.bool(), pred.shape)\n",
    "\n",
    "        if debug:\n",
    "            print(batch)  \n",
    "            print(f\"{test_mask.sum()=}\")\n",
    "            print(\n",
    "                f\"{pred.shape=} \\n\"\n",
    "                f\"{pred[test_mask].mean()=} \\n\"\n",
    "                f\"{pred[test_mask].max()=} \\n\"\n",
    "                f\"{pred[test_mask].min()=} \\n\"\n",
    "                f\"{pred[test_mask].shape=} \\n\"\n",
    "            ) \n",
    "        \n",
    "        loss = loss_fn(\n",
    "            torch.squeeze(torch.nan_to_num(pred[test_mask])), \n",
    "            torch.squeeze(torch.nan_to_num(batch.y.float()[torch.squeeze(test_mask)]))\n",
    "        )\n",
    "\n",
    "         # Update tracking\n",
    "        running_loss += loss.item()\n",
    "        step += 1\n",
    "        all_preds_raw.append(torch.nan_to_num(pred[test_mask]).cpu().detach().numpy())\n",
    "        all_labels.append(torch.nan_to_num(batch.y[torch.squeeze(test_mask)]).cpu().detach().numpy())\n",
    "        # all_knockout_ids.append(batch['enzymes'].knockout_label_id.cpu().detach().numpy())\n",
    "    \n",
    "    all_preds_raw = np.concatenate(all_preds_raw).ravel()\n",
    "    all_labels = np.concatenate(all_labels).ravel()\n",
    "    # all_knockout_ids = np.concatenate(all_knockout_ids).ravel()\n",
    "    log_metrics(all_preds_raw, all_labels, all_knockout_ids, epoch, \"test\")\n",
    "    return running_loss/step\n",
    "\n",
    "def build_model(model_config, output_size, debug=False):\n",
    "    if debug:\n",
    "        print(f\"creating model {model_config=}\")\n",
    "    # Set the number of outputs we expect\n",
    "    model_config['output_size'] = output_size\n",
    "    params = model_config\n",
    "\n",
    "    if 'mlflow' in model_config:\n",
    "        run_id = model_config['mlflow']['tags']['mlflow.parentRunId']\n",
    "        mlflow.set_tag(\"mlflow.parentRunId\", run_id)\n",
    "    # Logging params\n",
    "    for key in params.keys():\n",
    "        mlflow.log_param(key, params[key])\n",
    "  \n",
    "    # Loading the model\n",
    "    print(\"Loading model...\")\n",
    "    model = GATModel(model_config=params)\n",
    "    model = model.to(device)\n",
    "\n",
    "    return model\n",
    "\n",
    "@mlflow_mixin\n",
    "def run_one_training(model_config, train_samples, test_samples, checkpoint_dir):\n",
    "    # Build the model\n",
    "    model = build_model(model_config, train_samples[0].y.shape[0])\n",
    "\n",
    "    # Preparing training\n",
    "    train_loader = GeoDataLoader(train_samples, batch_size=model_config['batch_size'])\n",
    "    test_loader = GeoDataLoader(test_samples, batch_size=1)\n",
    "\n",
    "    # < 1 increases precision, > 1 recall\n",
    "    # loss_fn = torch.nn.MSELoss(reduction='none')\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    # we need to keep the lr quite low since otherwise the weights explode\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), \n",
    "        lr=model_config['learning_rate'],\n",
    "        momentum=model_config['sgd_momentum'],\n",
    "        # weight_decay=5e-4\n",
    "    )\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=model_config['scheduler_gamma'])\n",
    "\n",
    "    # Use this to debug the train and test function\n",
    "    debug = False\n",
    "    \n",
    "    # Start training\n",
    "    best_loss = 1000\n",
    "    early_stopping_counter = 0\n",
    "    max_epochs = 300\n",
    "    for epoch in tqdm(range(max_epochs)): \n",
    "        if early_stopping_counter <= 25: # = x * 5 \n",
    "            # Training\n",
    "            model.train()\n",
    "            loss = train_one_epoch(epoch, model, train_loader, optimizer, loss_fn, debug=debug)\n",
    "            # print(f\"Epoch {epoch} | Train Loss {loss}\")\n",
    "            mlflow.log_metric(key=\"Train loss\", value=float(loss), step=epoch)\n",
    "            \n",
    "            # Testing\n",
    "            model.eval()\n",
    "            if epoch % 5 == 0 or epoch == max_epochs - 1:\n",
    "                loss = test(epoch, model, test_loader, loss_fn, debug=debug)\n",
    "                # print(f\"Epoch {epoch} | Test Loss {loss}\")\n",
    "                mlflow.log_metric(key=\"Test loss\", value=float(loss), step=epoch)\n",
    "                \n",
    "                # Update best loss\n",
    "                if float(loss) < best_loss:\n",
    "                    best_loss = loss\n",
    "                    # Save the currently best model \n",
    "                    # mlflow.pytorch.log_model(model, \"model\", signature=SIGNATURE)\n",
    "                    early_stopping_counter = 0\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "\n",
    "            scheduler.step()\n",
    "            mlflow.log_metric(key=\"Learning rate\", value=float(scheduler.get_last_lr()[0]), step=epoch)\n",
    "            \n",
    "        else:\n",
    "            print(\"Early stopping due to no improvement.\")\n",
    "            session.report({\n",
    "                \"loss\": best_loss\n",
    "            })\n",
    "            return {\"loss\": best_loss}\n",
    "    print(f\"Finishing training with best test loss: {best_loss}\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sample = train_samples[0].to(device)\n",
    "        model.forward(sample.x_dict, sample.edge_index_dict)\n",
    "        print(f\"Number of parameters: {count_parameters(model)}\")\n",
    "\n",
    "    session.report({\n",
    "        \"loss\": best_loss\n",
    "    })\n",
    "\n",
    "    mlflow.end_run()\n",
    "\n",
    "    return {\"loss\": best_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETERS = {\n",
    "    \"batch_size\": tune.choice([2, 4, 8]),\n",
    "    \"learning_rate\": tune.choice([0.1, 0.05, 0.01, 0.001]),\n",
    "    \"sgd_momentum\": tune.choice([0.9, 0.8, 0.5]),\n",
    "    \"scheduler_gamma\": tune.choice([0.995, 1]),\n",
    "    \"model_embedding_size\": tune.choice([8, 16, 32, 64, 128]),\n",
    "    \"model_attention_heads\": tune.choice([1, 2, 3, 4]),\n",
    "    \"model_layers\": tune.choice([1, 3, 5, 7]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set\t [48 22 25  8  8 57 54 41  5 52]\n",
      "test set \t [23 44 56 16 23 51  5 46 43  2]\n",
      "val set  \t [62 47  2 14 14 44 42 30 68 35]\n",
      "train mask \t tensor([1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "test mask  \t tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0])\n",
      "val mask   \t tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])\n",
      "Epoch: 10, Loss: 1.9115\n",
      "Epoch: 20, Loss: 1.4799\n",
      "Epoch: 30, Loss: 1.1357\n",
      "Epoch: 40, Loss: 0.9913\n",
      "Epoch: 50, Loss: 0.8774\n",
      "Epoch: 60, Loss: 0.8635\n",
      "Epoch: 70, Loss: 0.7983\n",
      "Epoch: 80, Loss: 0.7897\n",
      "Epoch: 90, Loss: 0.8029\n",
      "Epoch: 100, Loss: 0.7558\n",
      "train set\t [12 17  7 56 60 47  5 72 48 62]\n",
      "test set \t [21 19 50 45 52 31 16 63 10 23]\n",
      "val set  \t [ 3  6 55 38  1 66 31 35 67 27]\n",
      "train mask \t tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1])\n",
      "test mask  \t tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "val mask   \t tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "Epoch: 10, Loss: 1.9326\n",
      "Epoch: 20, Loss: 1.3530\n",
      "Epoch: 30, Loss: 1.0757\n",
      "Epoch: 40, Loss: 0.9186\n",
      "Epoch: 50, Loss: 0.8532\n",
      "Epoch: 60, Loss: 0.8029\n",
      "Epoch: 70, Loss: 0.7913\n",
      "Epoch: 80, Loss: 0.7746\n",
      "Epoch: 90, Loss: 0.7686\n",
      "Epoch: 100, Loss: 0.7652\n",
      "train set\t [157 104 133  39 144  34   2  82 107  52]\n",
      "test set \t [ 24  42  87   5  31 111  51  54 130 142]\n",
      "val set  \t [ 39  36 135  58  32  38  67   3 154 137]\n",
      "train mask \t tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "test mask  \t tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n",
      "val mask   \t tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])\n",
      "Epoch: 10, Loss: 1.5531\n",
      "Epoch: 20, Loss: 1.0994\n",
      "Epoch: 30, Loss: 0.8921\n",
      "Epoch: 40, Loss: 0.8298\n",
      "Epoch: 50, Loss: 0.8011\n",
      "Epoch: 60, Loss: 0.7856\n",
      "Epoch: 70, Loss: 0.7787\n",
      "Epoch: 80, Loss: 0.7688\n",
      "Epoch: 90, Loss: 0.7681\n",
      "Epoch: 100, Loss: 0.7661\n"
     ]
    }
   ],
   "source": [
    "edge_list_df_unfiltered, graph_fc_unfiltered, edge_index_unfiltered, embedding_unfiltered = prepare_data(\n",
    "    './data/training/edge_list_unfiltered_protein_only.csv',\n",
    "    PRECURSOR_METABOLITES_NO_TRANSFORM,\n",
    ")\n",
    "edge_list_df_strict, graph_fc_strict, edge_index_strict, embedding_strict = prepare_data(\n",
    "    './data/training/edge_list_strict_protein_only.csv',\n",
    "    PRECURSOR_METABOLITES,\n",
    ")\n",
    "edge_list_df_all, graph_fc_all, edge_index_all, embedding_all = prepare_data(\n",
    "    './data/training/edge_list_all_protein_only.csv',\n",
    "    METABOLITES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_split(mode, metabolite_id, strategy):\n",
    "    if mode == 'unfiltered':\n",
    "        train_samples, test_samples = get_samples_graph(\n",
    "            metabolite_id, strategy, PRECURSOR_METABOLITES_NO_TRANSFORM, graph_fc_unfiltered, edge_index_unfiltered, embedding_unfiltered\n",
    "        )\n",
    "    elif mode == 'strict':\n",
    "        train_samples, test_samples = get_samples_graph(\n",
    "            target_metabolite_id=metabolite_id, strategy=strategy, valid_metabolites=PRECURSOR_METABOLITES, graph_fc_df=graph_fc_strict, edge_index=edge_index_strict, node_embeddings=embedding_strict,\n",
    "        )\n",
    "    elif mode == 'all':\n",
    "        train_samples, test_samples = get_samples_graph(\n",
    "            metabolite_id, strategy, METABOLITES, graph_fc_all, edge_index_all, embedding_all\n",
    "        )\n",
    "    return train_samples, test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()\n",
    "train_samples, test_samples = get_train_test_split('unfiltered', 'pyr', Strategy.ALL)\n",
    "print(test_samples[0])\n",
    "\n",
    "model_config = {\n",
    "    \"batch_size\": 6,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"sgd_momentum\": 0.8,\n",
    "    \"scheduler_gamma\": 1,\n",
    "    \"model_embedding_size\": 16,\n",
    "    \"model_attention_heads\": 1,\n",
    "    \"model_layers\": 1,\n",
    "}\n",
    "\n",
    "run_one_training(model_config, train_samples, test_samples, None)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training metabolite_id='accoa'\n",
      "95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 14:35:49,042\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "/home/tvangraft/tudelft/thesis/.env/lib/python3.8/site-packages/ray/tune/trainable/function_trainable.py:610: DeprecationWarning: `checkpoint_dir` in `func(config, checkpoint_dir)` is being deprecated. To save and load checkpoint in trainable functions, please use the `ray.air.session` API:\n",
      "\n",
      "from ray.air import session\n",
      "\n",
      "def train(config):\n",
      "    # ...\n",
      "    session.report({\"metric\": metric}, checkpoint=checkpoint)\n",
      "\n",
      "For more information please see https://docs.ray.io/en/master/tune/api_docs/trainable.html\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()\n",
    "for strategy in [Strategy.ONE_VS_ALL, Strategy.METABOLITE_CENTRIC]:\n",
    "    for mode, graph_fc in [('unfiltered', graph_fc_unfiltered), ('strict', graph_fc_strict), ('all', graph_fc_all)]:\n",
    "    # for mode, graph_fc in [('unfiltered', graph_fc_unfiltered)]:\n",
    "        run_name = \"model_gat_node_embeddings\"\n",
    "        experiment_name = f'protein_only_sweep_{mode}'\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "        with mlflow.start_run(run_name=run_name) as run:\n",
    "            for metabolite_id in list(set(graph_fc.columns.difference(ENZYMES).to_list()) & set(PRECURSOR_METABOLITES)):\n",
    "                print(f\"training {metabolite_id=}\")\n",
    "                train_samples, test_samples = get_train_test_split(mode, metabolite_id, strategy)\n",
    "                print(len(train_samples))\n",
    "\n",
    "                with mlflow.start_run(run_name=f\"model_{metabolite_id}_{strategy}\", nested=True):\n",
    "                    result = tune_metabolite_hyper_parameters(experiment_name, HYPERPARAMETERS, train_samples, test_samples, run_fn=run_one_training, num_samples=16)\n",
    "                    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a74f4189e970cd44747ccfc5ed5d3033ccfe233900630c96b00665fe147dda6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
